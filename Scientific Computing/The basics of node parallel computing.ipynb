{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "julia-1.5",
   "display_name": "Julia 1.5.0",
   "language": "julia"
  },
  "metadata": {
   "interpreter": {
    "hash": "177b667bbd2120d0945f6a3067172dc7a8ac4c3cb077d3ec6208cb503131f1af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 20.102833 seconds (617 allocations: 18.125 KiB)\n"
     ]
    }
   ],
   "source": [
    "@time for i in 1:10 \n",
    "sleep(2)\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  0.007297 seconds (6.23 k allocations: 370.582 KiB)\n"
     ]
    }
   ],
   "source": [
    "@time for i in 1:10 \n",
    "     @async sleep(2)\n",
    "end "
   ]
  },
  {
   "source": [
    "Notice that by adding @async we have reduced the time. We have added threads which do the calculations parallely and hence time required is very less."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[36m  @async\u001b[39m\n",
       "\n",
       "  Wrap an expression in a \u001b[36mTask\u001b[39m and add it to the local machine's scheduler\n",
       "  queue.\n",
       "\n",
       "  Values can be interpolated into \u001b[36m@async\u001b[39m via \u001b[36m$\u001b[39m, which copies the value\n",
       "  directly into the constructed underlying closure. This allows you to insert\n",
       "  the \u001b[4mvalue\u001b[24m of a variable, isolating the aysnchronous code from changes to the\n",
       "  variable's value in the current task.\n",
       "\n",
       "\u001b[39m\u001b[1m  │ \u001b[22m\u001b[39m\u001b[1mJulia 1.4\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m  Interpolating values via \u001b[36m$\u001b[39m is available as of Julia 1.4."
      ],
      "text/markdown": "```\n@async\n```\n\nWrap an expression in a [`Task`](@ref) and add it to the local machine's scheduler queue.\n\nValues can be interpolated into `@async` via `$`, which copies the value directly into the constructed underlying closure. This allows you to insert the *value* of a variable, isolating the aysnchronous code from changes to the variable's value in the current task.\n\n!!! compat \"Julia 1.4\"\n    Interpolating values via `$` is available as of Julia 1.4.\n\n",
      "text/latex": "\\begin{verbatim}\n@async\n\\end{verbatim}\nWrap an expression in a \\href{@ref}{\\texttt{Task}} and add it to the local machine's scheduler queue.\n\nValues can be interpolated into \\texttt{@async} via \\texttt{\\$}, which copies the value directly into the constructed underlying closure. This allows you to insert the \\emph{value} of a variable, isolating the aysnchronous code from changes to the variable's value in the current task.\n\n\\begin{quote}\n\\textbf{compat}\n\nJulia 1.4\n\nInterpolating values via \\texttt{\\$} is available as of Julia 1.4.\n\n\\end{quote}\n"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "?@async"
   ]
  },
  {
   "source": [
    "# @async and @sync\n",
    "Refer  [Site](https://riptutorial.com/julia-lang/example/15919/-async-and--sync)\n",
    "* According to the documentation under ?@async, \"@async wraps an expression in a Task.\" What this means is that for whatever falls within its scope, Julia will start this task running but then proceed to whatever comes next in the script without waiting for the task to complete. Thus, for instance, without the macro you will get:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  2.009118 seconds (64 allocations: 1.984 KiB)\n"
     ]
    }
   ],
   "source": [
    "@time sleep(2)"
   ]
  },
  {
   "source": [
    "But with the macro, you get:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  0.000066 seconds (26 allocations: 2.250 KiB)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "@time @async sleep(2)\n"
   ]
  },
  {
   "source": [
    "Julia thus allows the script to proceed (and the @time macro to fully execute) without waiting for the task (in this case, sleeping for two seconds) to complete.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The @sync macro, by contrast, will \"Wait until all dynamically-enclosed uses of @async, @spawn, @spawnat and @parallel are complete.\" (according to the documentation under ?@sync). Thus, we see:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  2.045602 seconds (1.34 k allocations: 70.828 KiB)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Task (done) @0x000000003b4a2b30"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "@time @sync @async sleep(2)"
   ]
  },
  {
   "source": [
    "In this simple example then, there is no point to including a single instance of @async and @sync together. But, where @sync can be useful is where you have @async applied to multiple operations that you wish to allow to all start at once without waiting for each to complete.\n",
    "\n",
    "For example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  0.000058 seconds (58 allocations: 8.734 KiB)\n",
      "  2.110324 seconds (1.65 k allocations: 93.312 KiB)\n"
     ]
    }
   ],
   "source": [
    "@time @sync @time for i in 1:10\n",
    "    @async sleep(2)\n",
    "    end"
   ]
  },
  {
   "source": [
    "So 0.000058 seconds is the time required to generate the threads and so all operations now are run **parallely** and hence take ~2 seconds.\n",
    "\n",
    "Notice that the above is actually **concurrency** since there is no computation involved."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Examples of the Differences\n",
    "Synchronous = Thread will complete an action Blocking = Thread will wait until action is completed\n",
    "\n",
    "* Asynchronous + Non-Blocking: I/O\n",
    "\n",
    "* Asynchronous + Blocking: Threaded atomics (demonstrated next lecture)\n",
    "\n",
    "* Synchronous + Blocking: Standard computing, @sync\n",
    "\n",
    "* Synchronous + Non-Blocking: Webservers where an I/O operation can be performed, but one never checks if the operation is completed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Multithreading\n",
    "If your threads are independent, then it may make sense to run them in parallel. This is the form of parallelism known as multithreading. To understand the data that is available in a multithreaded setup, let's look at the picture of threads again:\n",
    "![img](https://blog-assets.risingstack.com/2017/02/kernel-processes-and-threads-1.png)\n",
    "\n",
    "\n",
    "Each thread has its own call stack, but it's the process that holds the heap. This means that dynamically-sized heap allocated objects are shared between threads with no cost, a setup known as shared-memory computing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Loop-Based Multithreading with @threads\n",
    "Let's look back at our Lorenz dynamical system from before:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  4.700 μs (0 allocations: 0 bytes)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000-element Array{SArray{Tuple{3},Float64,1,3},1}:\n",
       " [1.0, 0.0, 0.0]\n",
       " [0.8, 0.56, 0.0]\n",
       " [0.752, 0.9968000000000001, 0.008960000000000001]\n",
       " [0.80096, 1.3978492416000001, 0.023474005333333336]\n",
       " [0.92033784832, 1.8180538219817644, 0.04461448495326095]\n",
       " [1.099881043052353, 2.296260732619613, 0.07569952060880669]\n",
       " [1.339156980965805, 2.864603692722823, 0.12217448583728006]\n",
       " [1.6442463233172087, 3.5539673118971193, 0.19238159391549564]\n",
       " [2.026190521033191, 4.397339452147425, 0.2989931959555302]\n",
       " [2.5004203072560376, 5.431943011293093, 0.4612438424853632]\n",
       " [3.0867248480634486, 6.700473453723668, 0.7082869831520391]\n",
       " [3.8094745691954923, 8.25130415895562, 1.0841620354518975]\n",
       " [4.697840487147518, 10.136982080467158, 1.655002727352565]\n",
       " ⋮\n",
       " [10.49730559336705, 4.660822889495989, 35.336831929448614]\n",
       " [9.330009052592839, 3.0272670946941713, 34.430722536963344]\n",
       " [8.069460661013105, 1.766747763108672, 33.159305922954225]\n",
       " [6.8089180814322185, 0.8987564841782779, 31.6759436385101]\n",
       " [5.6268857619814305, 0.3801973723631693, 30.108951163308078]\n",
       " [4.577548084057778, 0.13525687944525802, 28.545926978224173]\n",
       " [3.6890898431352737, 0.08257160199224252, 27.035860436772758]\n",
       " [2.9677861949066675, 0.15205611935372762, 25.600040161309696]\n",
       " [2.4046401797960795, 0.2914663505185634, 24.24373008707723]\n",
       " [1.9820054139405763, 0.46628657468365653, 22.964748583050085]\n",
       " [1.6788616460891923, 0.6565587545689172, 21.758445642263496]\n",
       " [1.4744010677851374, 0.8530017039412324, 20.62004063423844]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "using StaticArrays, BenchmarkTools\n",
    "function lorenz(u,p)\n",
    "  α,σ,ρ,β = p\n",
    "  @inbounds begin\n",
    "    du1 = u[1] + α*(σ*(u[2]-u[1]))\n",
    "    du2 = u[2] + α*(u[1]*(ρ-u[3]) - u[2])\n",
    "    du3 = u[3] + α*(u[1]*u[2] - β*u[3])\n",
    "  end\n",
    "  @SVector [du1,du2,du3]\n",
    "end\n",
    "function solve_system_save!(u,f,u0,p,n)\n",
    "  @inbounds u[1] = u0\n",
    "  @inbounds for i in 1:length(u)-1\n",
    "    u[i+1] = f(u[i],p)\n",
    "  end\n",
    "  u\n",
    "end\n",
    "p = (0.02,10.0,28.0,8/3)\n",
    "u = Vector{typeof(@SVector([1.0,0.0,0.0]))}(undef,1000)\n",
    "@btime solve_system_save!(u,lorenz,@SVector([1.0,0.0,0.0]),p,1000)"
   ]
  },
  {
   "source": [
    "In order to use multithreading on this code, we need to take a look at the dependency graph and see what items can be calculated independently of each other. Notice that\n",
    "\n",
    "```math\n",
    "σ*(u[2]-u[1])\n",
    "ρ-u[3]\n",
    "u[1]*u[2]\n",
    "β*u[3]\n",
    "```\n",
    "are all independent operations, so in theory we could split those off to different threads, move up, etc.\n",
    "\n",
    "Or we can have three threads:\n",
    "\n",
    "```math \n",
    "u[1] + α*(σ*(u[2]-u[1]))\n",
    "u[2] + α*(u[1]*(ρ-u[3]) - u[2])\n",
    "u[3] + α*(u[1]*u[2] - β*u[3])\n",
    "```\n",
    "all don't depend on the output of each other, so these tasks can be run in parallel. We can do this by using Julia's Threads.@threads macro which puts each of the computations of a loop in a different thread. The threaded loops do not allow you to return a value, so how do you build up the values for the @SVector?\n",
    "\n",
    "\n",
    "It's not possible!\n",
    "\n",
    "\n",
    "There is a shared heap, but the stacks are thread local. This means that a value cannot be stack allocated in one thread and magically appear when re-entering the main thread: it needs to go on the heap somewhere. But if it needs to go onto the heap, then it makes sense for us to have preallocated its location. But if we want to preallocate $du[1], du[2], and du[3]$, then it makes sense to use the fully non-allocating update form:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  6.100 μs (1 allocation: 112 bytes)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000-element Array{Array{Float64,1},1}:\n",
       " [1.0, 0.0, 0.0]\n",
       " [0.8, 0.56, 0.0]\n",
       " [0.752, 0.9968000000000001, 0.008960000000000001]\n",
       " [0.80096, 1.3978492416000001, 0.023474005333333336]\n",
       " [0.92033784832, 1.8180538219817644, 0.04461448495326095]\n",
       " [1.099881043052353, 2.296260732619613, 0.07569952060880669]\n",
       " [1.339156980965805, 2.864603692722823, 0.12217448583728006]\n",
       " [1.6442463233172087, 3.5539673118971193, 0.19238159391549564]\n",
       " [2.026190521033191, 4.397339452147425, 0.2989931959555302]\n",
       " [2.5004203072560376, 5.431943011293093, 0.4612438424853632]\n",
       " [3.0867248480634486, 6.700473453723668, 0.7082869831520391]\n",
       " [3.8094745691954923, 8.25130415895562, 1.0841620354518975]\n",
       " [4.697840487147518, 10.136982080467158, 1.655002727352565]\n",
       " ⋮\n",
       " [10.49730559336705, 4.660822889495989, 35.336831929448614]\n",
       " [9.330009052592839, 3.0272670946941713, 34.430722536963344]\n",
       " [8.069460661013105, 1.766747763108672, 33.159305922954225]\n",
       " [6.8089180814322185, 0.8987564841782779, 31.6759436385101]\n",
       " [5.6268857619814305, 0.3801973723631693, 30.108951163308078]\n",
       " [4.577548084057778, 0.13525687944525802, 28.545926978224173]\n",
       " [3.6890898431352737, 0.08257160199224252, 27.035860436772758]\n",
       " [2.9677861949066675, 0.15205611935372762, 25.600040161309696]\n",
       " [2.4046401797960795, 0.2914663505185634, 24.24373008707723]\n",
       " [1.9820054139405763, 0.46628657468365653, 22.964748583050085]\n",
       " [1.6788616460891923, 0.6565587545689172, 21.758445642263496]\n",
       " [1.4744010677851374, 0.8530017039412324, 20.62004063423844]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "function lorenz!(du,u,p)\n",
    "  α,σ,ρ,β = p\n",
    "  @inbounds begin\n",
    "    du[1] = u[1] + α*(σ*(u[2]-u[1]))\n",
    "    du[2] = u[2] + α*(u[1]*(ρ-u[3]) - u[2])\n",
    "    du[3] = u[3] + α*(u[1]*u[2] - β*u[3])\n",
    "  end\n",
    "end\n",
    "function solve_system_save_iip!(u,f,u0,p,n)\n",
    "  @inbounds u[1] = u0\n",
    "  @inbounds for i in 1:length(u)-1\n",
    "    f(u[i+1],u[i],p)\n",
    "  end\n",
    "  u\n",
    "end\n",
    "p = (0.02,10.0,28.0,8/3)\n",
    "u = [Vector{Float64}(undef,3) for i in 1:1000]\n",
    "@btime solve_system_save_iip!(u,lorenz!,[1.0,0.0,0.0],p,1000)"
   ]
  },
  {
   "source": [
    "and now we multithread:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  1.924 ms (5995 allocations: 967.89 KiB)\n"
     ]
    }
   ],
   "source": [
    "using Base.Threads\n",
    "function lorenz_mt!(du,u,p)\n",
    "  α,σ,ρ,β = p\n",
    "  let du=du, u=u, p=p\n",
    "    Threads.@threads for i in 1:3\n",
    "      @inbounds begin\n",
    "        if i == 1\n",
    "          du[1] = u[1] + α*(σ*(u[2]-u[1]))\n",
    "        elseif i == 2\n",
    "          du[2] = u[2] + α*(u[1]*(ρ-u[3]) - u[2])\n",
    "        else\n",
    "          du[3] = u[3] + α*(u[1]*u[2] - β*u[3])\n",
    "        end\n",
    "        nothing\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "  nothing\n",
    "end\n",
    "function solve_system_save_iip!(u,f,u0,p,n)\n",
    "  @inbounds u[1] = u0\n",
    "  @inbounds for i in 1:length(u)-1\n",
    "    f(u[i+1],u[i],p)\n",
    "  end\n",
    "  u\n",
    "end\n",
    "p = (0.02,10.0,28.0,8/3)\n",
    "u = [Vector{Float64}(undef,3) for i in 1:1000]\n",
    "@btime solve_system_save_iip!(u,lorenz_mt!,[1.0,0.0,0.0],p,1000);"
   ]
  },
  {
   "source": [
    "**Parallelism doesn't always make things faster**\n",
    " There are two costs associated with this code. For one, we had to go to the slower heap+mutation version, so its implementation starting point is slower. But secondly, and more importantly, the cost of spinning a new thread is non-negligable. In fact, here we can see that it even needs to make a small allocation for the new context. The total cost is on the order of It's on the order of 50ns: not huge, but something to take note of. So what we've done is taken almost free calculations and made them ~50ns by making each in a different thread, instead of just having it be one thread with one call stack.\n",
    "\n",
    "The moral of the story is that you need to make sure that there's enough work per thread in order to effectively accelerate a program with parallelism."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000-element Array{SArray{Tuple{3},Float64,1,3},1}:\n",
       " [6.4e-323, 5.152288944e-315, 2.27e-322]\n",
       " [5.45569341e-315, NaN, 0.0]\n",
       " [0.0, -1.022081779e-314, 7.15014174e-316]\n",
       " [0.0, 8.4e-323, 4.642775625e-315]\n",
       " [0.0, 7.1501449e-316, 0.0]\n",
       " [8.4e-323, 4.642775625e-315, 5.0e-324]\n",
       " [4.77155874e-316, 0.0, 8.4e-323]\n",
       " [4.642775625e-315, 1.0e-323, 7.15014174e-316]\n",
       " [0.0, 8.4e-323, 4.642775625e-315]\n",
       " [1.5e-323, 0.0, -5.442527131007e-312]\n",
       " [0.0, 0.0, 4.649742074e-315]\n",
       " [0.0, 0.0, 4.176818213e-315]\n",
       " [0.0, 4.201675595e-315, 7.2111091e-316]\n",
       " ⋮\n",
       " [NaN, 2.1219957905e-314, 0.0]\n",
       " [0.0, 0.0, 0.0]\n",
       " [0.0, 5.455810763e-315, 8.487983164e-314]\n",
       " [0.0, 0.0, 0.0]\n",
       " [0.0, 0.0, NaN]\n",
       " [2.1219957905e-314, 0.0, 0.0]\n",
       " [0.0, 0.0, 0.0]\n",
       " [5.455811316e-315, 8.487983164e-314, 0.0]\n",
       " [0.0, 0.0, 0.0]\n",
       " [0.0, NaN, 2.1219957905e-314]\n",
       " [0.0, 0.0, 0.0]\n",
       " [0.0, 0.0, 5.45581187e-315]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "u = Vector{typeof(@SVector([1.0,0.0,0.0]))}(undef,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}